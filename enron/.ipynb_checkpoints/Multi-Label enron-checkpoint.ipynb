{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# show graphs in jupyter\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# display all columns/rows and what's inside\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/multi-label-text-classification-5c505fdedca8#--responses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "from itertools import cycle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from skmultilearn.ext import Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of metrics\n",
    "Evals = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-Label Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1702, 1054)\n"
     ]
    }
   ],
   "source": [
    "# Read data from file 'filename.csv' \n",
    "# (in the same directory that your python process is based)\n",
    "# Control delimiters, rows, column names with read_csv (see later) \n",
    "data = pd.read_csv(\"enron.csv\") \n",
    "# Preview the first 5 lines of the loaded data \n",
    "print(data.shape)\n",
    "# X_train, y_train\n",
    "y = data.iloc[:,-53:]\n",
    "X = data.iloc[:,:-53]\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Multiple Binary Classifications - (Binary Relevance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1  BinaryRelevance MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_single_class(input_dim, output_dim):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "KERAS_PARAMS = dict(epochs=10, batch_size=100, verbose=0)\n",
    "clf = BinaryRelevance(classifier=Keras(create_model_single_class, False, KERAS_PARAMS), require_dense=[True,True])\n",
    "clf.fit(X_train, y_train)\n",
    "result = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy   None, 'micro', 'macro', 'weighted', 'samples'\n",
    "Accuracy = accuracy_score(y_test,result.toarray())\n",
    "Micro_Precision = precision_score(y_test,result, average='micro')\n",
    "Micro_Recall = recall_score(y_test,result, average='micro')\n",
    "Micro_F1 = f1_score(y_test,result, average='micro')\n",
    "Micro_Average_Prec = average_precision_score(y_test, result.toarray(), average='micro')\n",
    "Macro_Precision = precision_score(y_test,result, average='macro')\n",
    "Macro_Recall = recall_score(y_test,result, average='macro')\n",
    "Macro_F1 = f1_score(y_test,result, average='macro')\n",
    "Macro_Average_Prec = average_precision_score(y_test, result.toarray(), average='macro')\n",
    "Samples_Average_Prec = average_precision_score(y_test, result.toarray(), average='samples')\n",
    "Weighted_Average_Prec = average_precision_score(y_test, result.toarray(), average='weighted')\n",
    "Hamming_loss = hamming_loss(y_test,result)\n",
    "Ranking_loss = label_ranking_loss(y_test, result.toarray())\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "#fpr[\"micro\"], tpr[\"micro\"], _  = roc_curve(y_test.toarray(), result)\n",
    "fpr, tpr, thresholds = roc_curve(y_test.ravel(), result.toarray().ravel())\n",
    "Micro_Roc_auc = auc(fpr, tpr)\n",
    "Coverage_error = coverage_error(y_test, result.toarray())\n",
    "label_ranking_avg_prec_score = label_ranking_average_precision_score(y_test, result.toarray())\n",
    "Jaccard_score = jaccard_score(y_test, result.toarray(), average='samples')\n",
    "Jaccard_score_macro = jaccard_score(y_test, result.toarray(), average='macro')\n",
    "print(\"BinaryReMLP Accuracy = \",Accuracy)\n",
    "print(\"BinaryReMLP Micro_Precision = \",Micro_Precision)\n",
    "print(\"BinaryReMLP Micro_Recall = \",Micro_Recall)\n",
    "print(\"BinaryReMLP Micro_F1 = \",Micro_F1)\n",
    "print(\"BinaryReMLP Micro_Average_Prec = \",Micro_Average_Prec)\n",
    "print(\"BinaryReMLP Macro_Precision = \",Macro_Precision)\n",
    "print(\"BinaryReMLP Macro_Recall = \",Macro_Recall)\n",
    "print(\"BinaryReMLP Macro_F1 = \",Macro_F1)\n",
    "print(\"BinaryReMLP Macro_Average_Prec = \",Macro_Average_Prec)\n",
    "print(\"BinaryReMLP Samples_Average_Prec = \",Samples_Average_Prec)\n",
    "print(\"BinaryReMLP Weighted_Average_Prec = \",Weighted_Average_Prec)\n",
    "print(\"BinaryReMLP Hamming_loss = \",Hamming_loss)\n",
    "print(\"BinaryReMLP Ranking_loss = \",Ranking_loss)\n",
    "print(\"BinaryReMLP Micro_Roc_auc = \",Micro_Roc_auc)\n",
    "print(\"BinaryReMLP Coverage_error = \",Coverage_error)\n",
    "print(\"BinaryReMLP label_ranking_average_precision_score = \",label_ranking_avg_prec_score)\n",
    "print(\"BinaryReMLP Jaccard_score = \",Jaccard_score)\n",
    "print(\"BinaryReMLP Jaccard_score_macro = \",Jaccard_score_macro)\n",
    "Eval1 = ['BinaryReMLP',Accuracy,Micro_Precision,Micro_Recall,Micro_F1,Micro_Average_Prec,\n",
    "         Macro_Precision,Macro_Recall,Macro_F1,Macro_Average_Prec,Samples_Average_Prec,Weighted_Average_Prec,Hamming_loss,\n",
    "         Ranking_loss,Micro_Roc_auc,Coverage_error,label_ranking_avg_prec_score,Jaccard_score,Jaccard_score_macro]\n",
    "Evals.append(Eval1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2  BinaryRelevance SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Binary Relevance multi-label classifier\n",
    "# with an SVM classifier  # SVM in scikit only supports the X matrix in sparse representation\n",
    "# Setup the classifier\n",
    "classifier = BinaryRelevance(classifier=SVC(), require_dense=[False,True])\n",
    "# Train\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predict\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy   None, 'micro', 'macro', 'weighted', 'samples'\n",
    "# In multilabel classification, this function computes subset_accuracy:\n",
    "Accuracy = accuracy_score(y_test,y_pred)  \n",
    "Micro_Precision = precision_score(y_test,y_pred, average='micro')\n",
    "Micro_Recall = recall_score(y_test,y_pred, average='micro')\n",
    "Micro_F1 = f1_score(y_test,y_pred, average='micro')\n",
    "Micro_Average_Prec = average_precision_score(y_test, y_pred.toarray(), average='micro')\n",
    "Macro_Precision = precision_score(y_test,y_pred, average='macro')\n",
    "Macro_Recall = recall_score(y_test,y_pred, average='macro')\n",
    "Macro_F1 = f1_score(y_test,y_pred, average='macro')\n",
    "Macro_Average_Prec = average_precision_score(y_test, y_pred.toarray(), average='macro')\n",
    "Samples_Average_Prec = average_precision_score(y_test, y_pred.toarray(), average='samples')\n",
    "Weighted_Average_Prec = average_precision_score(y_test, y_pred.toarray(), average='weighted')\n",
    "Hamming_loss = hamming_loss(y_test,y_pred)\n",
    "Ranking_loss = label_ranking_loss(y_test, y_pred.toarray())\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "#fpr[\"micro\"], tpr[\"micro\"], _  = roc_curve(y_test, y_pred)\n",
    "fpr, tpr, thresholds = roc_curve(y_test.ravel(), y_pred.toarray().ravel())\n",
    "Micro_Roc_auc = auc(fpr, tpr)\n",
    "Coverage_error = coverage_error(y_test, y_pred.toarray())\n",
    "label_ranking_avg_prec_score = label_ranking_average_precision_score(y_test, y_pred.toarray())\n",
    "Jaccard_score = jaccard_score(y_test, y_pred, average='samples')\n",
    "Jaccard_score_macro = jaccard_score(y_test, y_pred, average='macro')\n",
    "print(\"BinaryReSVC Accuracy = \",Accuracy)\n",
    "print(\"BinaryReSVC Micro_Precision = \",Micro_Precision)\n",
    "print(\"BinaryReSVC Micro_Recall = \",Micro_Recall)\n",
    "print(\"BinaryReSVC Micro_F1 = \",Micro_F1)\n",
    "print(\"BinaryReSVC Micro_Average_Prec = \",Micro_Average_Prec)\n",
    "print(\"BinaryReSVC Macro_Precision = \",Macro_Precision)\n",
    "print(\"BinaryReSVC Macro_Recall = \",Macro_Recall)\n",
    "print(\"BinaryReSVC Macro_F1 = \",Macro_F1)\n",
    "print(\"BinaryReSVC Macro_Average_Prec = \",Macro_Average_Prec)\n",
    "print(\"BinaryReSVC Samples_Average_Prec = \",Samples_Average_Prec)\n",
    "print(\"BinaryReSVC Weighted_Average_Prec = \",Weighted_Average_Prec)\n",
    "print(\"BinaryReSVC Hamming_loss = \",Hamming_loss)\n",
    "print(\"BinaryReSVC Ranking_loss = \",Ranking_loss)\n",
    "print(\"BinaryReSVC Micro_Roc_auc = \",Micro_Roc_auc)\n",
    "print(\"BinaryReSVC Coverage_error = \",Coverage_error)\n",
    "print(\"BinaryReSVC label_ranking_average_precision_score = \",label_ranking_avg_prec_score)\n",
    "print(\"BinaryReSVC Jaccard_score = \",Jaccard_score)\n",
    "print(\"BinaryReSVC Jaccard_score_macro = \",Jaccard_score_macro)\n",
    "Eval2 = ['BinaryReSVC',Accuracy,Micro_Precision,Micro_Recall,Micro_F1,Micro_Average_Prec,\n",
    "         Macro_Precision,Macro_Recall,Macro_F1,Macro_Average_Prec,Samples_Average_Prec,Weighted_Average_Prec,Hamming_loss,\n",
    "         Ranking_loss,Micro_Roc_auc,Coverage_error,label_ranking_avg_prec_score,Jaccard_score,Jaccard_score_macro]\n",
    "Evals.append(Eval2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3  BinaryRelevance GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary Relevance GaussianNB\n",
    "BinaryReGaussianNB = BinaryRelevance(GaussianNB())\n",
    "BinaryReGaussianNB.fit(X_train,y_train)\n",
    "br_predictions = BinaryReGaussianNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy   None, 'micro', 'macro', 'weighted', 'samples'\n",
    "Accuracy = accuracy_score(y_test,br_predictions.toarray())\n",
    "Micro_Precision = precision_score(y_test,br_predictions, average='micro')\n",
    "Micro_Recall = recall_score(y_test,br_predictions, average='micro')\n",
    "Micro_F1 = f1_score(y_test,br_predictions, average='micro')\n",
    "Micro_Average_Prec = average_precision_score(y_test, br_predictions.toarray(), average='micro')\n",
    "Macro_Precision = precision_score(y_test,br_predictions, average='macro')\n",
    "Macro_Recall = recall_score(y_test,br_predictions, average='macro')\n",
    "Macro_F1 = f1_score(y_test,br_predictions, average='macro')\n",
    "Macro_Average_Prec = average_precision_score(y_test, br_predictions.toarray(), average='macro')\n",
    "Samples_Average_Prec = average_precision_score(y_test, br_predictions.toarray(), average='samples')\n",
    "Weighted_Average_Prec = average_precision_score(y_test, br_predictions.toarray(), average='weighted')\n",
    "Hamming_loss = hamming_loss(y_test,br_predictions)\n",
    "Ranking_loss = label_ranking_loss(y_test, br_predictions.toarray())\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "#fpr[\"micro\"], tpr[\"micro\"], _  = roc_curve(y_test.toarray(), br_predictions)\n",
    "fpr, tpr, thresholds = roc_curve(y_test.ravel(), br_predictions.toarray().ravel())\n",
    "Micro_Roc_auc = auc(fpr, tpr)\n",
    "Coverage_error = coverage_error(y_test, br_predictions.toarray())\n",
    "label_ranking_avg_prec_score = label_ranking_average_precision_score(y_test, br_predictions.toarray())\n",
    "Jaccard_score = jaccard_score(y_test, br_predictions, average='samples')\n",
    "Jaccard_score_macro = jaccard_score(y_test, br_predictions, average='macro')\n",
    "print(\"BinaryReGaussianNB Accuracy = \",Accuracy)\n",
    "print(\"BinaryReGaussianNB Micro_Precision = \",Micro_Precision)\n",
    "print(\"BinaryReGaussianNB Micro_Recall = \",Micro_Recall)\n",
    "print(\"BinaryReGaussianNB Micro_F1 = \",Micro_F1)\n",
    "print(\"BinaryReGaussianNB Micro_Average_Prec = \",Micro_Average_Prec)\n",
    "print(\"BinaryReGaussianNB Macro_Precision = \",Macro_Precision)\n",
    "print(\"BinaryReGaussianNB Macro_Recall = \",Macro_Recall)\n",
    "print(\"BinaryReGaussianNB Macro_F1 = \",Macro_F1)\n",
    "print(\"BinaryReGaussianNB Macro_Average_Prec = \",Macro_Average_Prec)\n",
    "print(\"BinaryReGaussianNB Samples_Average_Prec = \",Samples_Average_Prec)\n",
    "print(\"BinaryReGaussianNB Weighted_Average_Prec = \",Weighted_Average_Prec)\n",
    "print(\"BinaryReGaussianNB Hamming_loss = \",Hamming_loss)\n",
    "print(\"BinaryReGaussianNB Ranking_loss = \",Ranking_loss)\n",
    "print(\"BinaryReGaussianNB Micro_Roc_auc = \",Micro_Roc_auc)\n",
    "print(\"BinaryReGaussianNB Coverage_error = \",Coverage_error)\n",
    "print(\"BinaryReGaussianNB label_ranking_average_precision_score = \",label_ranking_avg_prec_score)\n",
    "print(\"BinaryReGaussianNB Jaccard_score = \",Jaccard_score)\n",
    "print(\"BinaryReGaussianNB Jaccard_score_macro = \",Jaccard_score_macro)\n",
    "Eval3 = ['BinaryReGaussianNB',Accuracy,Micro_Precision,Micro_Recall,Micro_F1,Micro_Average_Prec,\n",
    "         Macro_Precision,Macro_Recall,Macro_F1,Macro_Average_Prec,Samples_Average_Prec,Weighted_Average_Prec,Hamming_loss,\n",
    "         Ranking_loss,Micro_Roc_auc,Coverage_error,label_ranking_avg_prec_score,Jaccard_score,Jaccard_score_macro]\n",
    "Evals.append(Eval3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Label Powerset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize label powerset multi-label classifier\n",
    "lp_classifier = LabelPowerset(LogisticRegression())\n",
    "lp_classifier.fit(X_train, y_train)\n",
    "lp_predictions = lp_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy   None, 'micro', 'macro', 'weighted', 'samples'\n",
    "Accuracy = accuracy_score(y_test,lp_predictions.toarray())\n",
    "Micro_Precision = precision_score(y_test,lp_predictions, average='micro')\n",
    "Micro_Recall = recall_score(y_test,lp_predictions, average='micro')\n",
    "Micro_F1 = f1_score(y_test,lp_predictions, average='micro')\n",
    "Micro_Average_Prec = average_precision_score(y_test, lp_predictions.toarray(), average='micro')\n",
    "Macro_Precision = precision_score(y_test,lp_predictions, average='macro')\n",
    "Macro_Recall = recall_score(y_test,lp_predictions, average='macro')\n",
    "Macro_F1 = f1_score(y_test,lp_predictions, average='macro')\n",
    "Macro_Average_Prec = average_precision_score(y_test, lp_predictions.toarray(), average='macro')\n",
    "Samples_Average_Prec = average_precision_score(y_test, lp_predictions.toarray(), average='samples')\n",
    "Weighted_Average_Prec = average_precision_score(y_test, lp_predictions.toarray(), average='weighted')\n",
    "Hamming_loss = hamming_loss(y_test,lp_predictions)\n",
    "Ranking_loss = label_ranking_loss(y_test, lp_predictions.toarray())\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "#fpr[\"micro\"], tpr[\"micro\"], _  = roc_curve(y_test, lp_predictions)\n",
    "fpr, tpr, thresholds = roc_curve(y_test.ravel(), lp_predictions.toarray().ravel())\n",
    "Micro_Roc_auc = auc(fpr, tpr)\n",
    "Coverage_error = coverage_error(y_test, lp_predictions.toarray())\n",
    "label_ranking_avg_prec_score = label_ranking_average_precision_score(y_test, lp_predictions.toarray())\n",
    "Jaccard_score = jaccard_score(y_test, lp_predictions, average='samples')\n",
    "Jaccard_score_macro = jaccard_score(y_test, lp_predictions, average='macro')\n",
    "print(\"LabelPowersetLogisticRegression Accuracy = \",Accuracy)\n",
    "print(\"LabelPowersetLogisticRegression Micro_Precision = \",Micro_Precision)\n",
    "print(\"LabelPowersetLogisticRegression Micro_Recall = \",Micro_Recall)\n",
    "print(\"LabelPowersetLogisticRegression Micro_F1 = \",Micro_F1)\n",
    "print(\"LabelPowersetLogisticRegression Micro_Average_Prec = \",Micro_Average_Prec)\n",
    "print(\"LabelPowersetLogisticRegression Macro_Precision = \",Macro_Precision)\n",
    "print(\"LabelPowersetLogisticRegression Macro_Recall = \",Macro_Recall)\n",
    "print(\"LabelPowersetLogisticRegression Macro_F1 = \",Macro_F1)\n",
    "print(\"LabelPowersetLogisticRegression Macro_Average_Prec = \",Macro_Average_Prec)\n",
    "print(\"LabelPowersetLogisticRegression Samples_Average_Prec = \",Samples_Average_Prec)\n",
    "print(\"LabelPowersetLogisticRegression Weighted_Average_Prec = \",Weighted_Average_Prec)\n",
    "print(\"LabelPowersetLogisticRegression Hamming_loss = \",Hamming_loss)\n",
    "print(\"LabelPowersetLogisticRegression Ranking_loss = \",Ranking_loss)\n",
    "print(\"LabelPowersetLogisticRegression Micro_Roc_auc = \",Micro_Roc_auc)\n",
    "print(\"LabelPowersetLogisticRegression Coverage_error = \",Coverage_error)\n",
    "print(\"LabelPowersetLogisticRegression label_ranking_average_precision_score = \",label_ranking_avg_prec_score)\n",
    "print(\"LabelPowersetLogisticRegression Jaccard_score = \",Jaccard_score)\n",
    "print(\"LabelPowersetLogisticRegression Jaccard_score_macro = \",Jaccard_score_macro)\n",
    "Eval4 = ['LabelPowersetLogisticRegression',Accuracy,Micro_Precision,Micro_Recall,Micro_F1,Micro_Average_Prec,\n",
    "         Macro_Precision,Macro_Recall,Macro_F1,Macro_Average_Prec,Samples_Average_Prec,Weighted_Average_Prec,Hamming_loss,\n",
    "         Ranking_loss,Micro_Roc_auc,Coverage_error,label_ranking_avg_prec_score,Jaccard_score,Jaccard_score_macro]\n",
    "Evals.append(Eval4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3. Adapted Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLkNN\n",
    "# Adapted Algorithm¶\n",
    "# http://scikit.ml/api/api/skmultilearn.adapt.html#skmultilearn.adapt.MLkNN\n",
    "ml_classifier = MLkNN(k=10)\n",
    "# to prevent errors when handling sparse matrices.\n",
    "X_train = lil_matrix(X_train).toarray()\n",
    "y_train = lil_matrix(y_train).toarray()\n",
    "X_test = lil_matrix(X_test).toarray()\n",
    "ml_classifier.fit(X_train, y_train)\n",
    "# predict\n",
    "ml_predictions = ml_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy   None, 'micro', 'macro', 'weighted', 'samples'\n",
    "Accuracy = accuracy_score(y_test,ml_predictions.toarray())\n",
    "Micro_Precision = precision_score(y_test,ml_predictions, average='micro')\n",
    "Micro_Recall = recall_score(y_test,ml_predictions, average='micro')\n",
    "Micro_F1 = f1_score(y_test,ml_predictions, average='micro')\n",
    "Micro_Average_Prec = average_precision_score(y_test, ml_predictions.toarray(), average='micro')\n",
    "Macro_Precision = precision_score(y_test,ml_predictions, average='macro')\n",
    "Macro_Recall = recall_score(y_test,ml_predictions, average='macro')\n",
    "Macro_F1 = f1_score(y_test,ml_predictions, average='macro')\n",
    "Macro_Average_Prec = average_precision_score(y_test, ml_predictions.toarray(), average='macro')\n",
    "Samples_Average_Prec = average_precision_score(y_test, ml_predictions.toarray(), average='samples')\n",
    "Weighted_Average_Prec = average_precision_score(y_test, ml_predictions.toarray(), average='weighted')\n",
    "Hamming_loss = hamming_loss(y_test,ml_predictions)\n",
    "Ranking_loss = label_ranking_loss(y_test, ml_predictions.toarray())\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "#fpr[\"micro\"], tpr[\"micro\"], _  = roc_curve(y_test, ml_predictions)\n",
    "fpr, tpr, thresholds = roc_curve(y_test.ravel(), ml_predictions.toarray().ravel())\n",
    "Micro_Roc_auc = auc(fpr, tpr)\n",
    "Coverage_error = coverage_error(y_test, ml_predictions.toarray())\n",
    "label_ranking_avg_prec_score = label_ranking_average_precision_score(y_test, ml_predictions.toarray())\n",
    "Jaccard_score = jaccard_score(y_test, ml_predictions, average='samples')\n",
    "Jaccard_score_macro = jaccard_score(y_test, ml_predictions, average='macro')\n",
    "print(\"MLkNN Accuracy = \",Accuracy)\n",
    "print(\"MLkNN Micro_Precision = \",Micro_Precision)\n",
    "print(\"MLkNN Micro_Recall = \",Micro_Recall)\n",
    "print(\"MLkNN Micro_F1 = \",Micro_F1)\n",
    "print(\"MLkNN Micro_Average_Prec = \",Micro_Average_Prec)\n",
    "print(\"MLkNN Macro_Precision = \",Macro_Precision)\n",
    "print(\"MLkNN Macro_Recall = \",Macro_Recall)\n",
    "print(\"MLkNN Macro_F1 = \",Macro_F1)\n",
    "print(\"MLkNN Macro_Average_Prec = \",Macro_Average_Prec)\n",
    "print(\"MLkNN Samples_Average_Prec = \",Samples_Average_Prec)\n",
    "print(\"MLkNN Weighted_Average_Prec = \",Weighted_Average_Prec)\n",
    "print(\"MLkNN Hamming_loss = \",Hamming_loss)\n",
    "print(\"MLkNN Ranking_loss = \",Ranking_loss)\n",
    "print(\"MLkNN Micro_Roc_auc = \",Micro_Roc_auc)\n",
    "print(\"MLkNN Coverage_error = \",Coverage_error)\n",
    "print(\"MLkNN label_ranking_average_precision_score = \",label_ranking_avg_prec_score)\n",
    "print(\"MLkNN Jaccard_score = \",Jaccard_score)\n",
    "print(\"MLkNN Jaccard_score_macro = \",Jaccard_score_macro)\n",
    "Eval5 = ['MLkNN',Accuracy,Micro_Precision,Micro_Recall,Micro_F1,Micro_Average_Prec,\n",
    "         Macro_Precision,Macro_Recall,Macro_F1,Macro_Average_Prec,Samples_Average_Prec,Weighted_Average_Prec,Hamming_loss,\n",
    "         Ranking_loss,Micro_Roc_auc,Coverage_error,label_ranking_avg_prec_score,Jaccard_score,Jaccard_score_macro]\n",
    "Evals.append(Eval5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Classifier Chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the next classifier we need to remove from y-train, y-test categories which equal 0 for all train samples\n",
    "cc_classifier = ClassifierChain(LogisticRegression(solver='warn'))\n",
    "cc_classifier.fit(X_train, y_train)\n",
    "cc_predictions_proba = cc_classifier.predict_proba(X_test)\n",
    "#for plotting metrics as a function of threashold\n",
    "th = []\n",
    "f = []\n",
    "ham = []\n",
    "ac = []\n",
    "for t in range (5,60): # threshold value\n",
    "    y_pred_new = (cc_predictions_proba >= t/100).astype(int)\n",
    "#     print(\"t =\" ,t/100)\n",
    "#     print(\"Accuracy = \",accuracy_score(y_test,y_pred_new))\n",
    "#     print(\"F1 = \",f1_score(y_test,y_pred_new, average=\"micro\"))\n",
    "#     print(\"Hamming loss = \",hamming_loss(y_test,y_pred_new))\n",
    "    th.append(t)\n",
    "    ac.append(accuracy_score(y_test,y_pred_new))\n",
    "    f.append(f1_score(y_test,y_pred_new, average=\"micro\"))\n",
    "    ham.append(hamming_loss(y_test,y_pred_new))\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "with plt.style.context('ggplot'):\n",
    "    plt.plot(th, f)\n",
    "    plt.plot(th, ham)\n",
    "    plt.plot(th, ac)\n",
    "    plt.legend(['F1', 'Hamming loss', 'Accuracy'], loc='center left', fontsize = 14)\n",
    "    plt.ylabel(\"metrics\", fontsize = 14)\n",
    "    plt.xlabel(\"threshold\", fontsize = 14)\n",
    "    plt.title(\"Classfier Chain Model\", fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using classifier chains\n",
    "# initialize classifier chains multi-label classifier\n",
    "classifier = ClassifierChain(LogisticRegression())\n",
    "# Training logistic regression model on train data\n",
    "classifier.fit(X_train, y_train)\n",
    "# predict\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy   None, 'micro', 'macro', 'weighted', 'samples'\n",
    "Accuracy = accuracy_score(y_test,predictions.toarray())\n",
    "Micro_Precision = precision_score(y_test,predictions, average='micro')\n",
    "Micro_Recall = recall_score(y_test,predictions, average='micro')\n",
    "Micro_F1 = f1_score(y_test,predictions, average='micro')\n",
    "Micro_Average_Prec = average_precision_score(y_test, predictions.toarray(), average='micro')\n",
    "Macro_Precision = precision_score(y_test,predictions, average='macro')\n",
    "Macro_Recall = recall_score(y_test,predictions, average='macro')\n",
    "Macro_F1 = f1_score(y_test,predictions, average='macro')\n",
    "Macro_Average_Prec = average_precision_score(y_test, predictions.toarray(), average='macro')\n",
    "Samples_Average_Prec = average_precision_score(y_test, predictions.toarray(), average='samples')\n",
    "Weighted_Average_Prec = average_precision_score(y_test, predictions.toarray(), average='weighted')\n",
    "Hamming_loss = hamming_loss(y_test,predictions)\n",
    "Ranking_loss = label_ranking_loss(y_test, predictions.toarray())\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "#fpr[\"micro\"], tpr[\"micro\"], _  = roc_curve(y_test, predictions)\n",
    "fpr, tpr, thresholds = roc_curve(y_test.ravel(), predictions.toarray().ravel())\n",
    "Micro_Roc_auc = auc(fpr, tpr)\n",
    "Coverage_error = coverage_error(y_test, predictions.toarray())\n",
    "label_ranking_avg_prec_score = label_ranking_average_precision_score(y_test, predictions.toarray())\n",
    "Jaccard_score = jaccard_score(y_test, predictions, average='samples')\n",
    "Jaccard_score_macro = jaccard_score(y_test, predictions, average='macro')\n",
    "print(\"ClassifierChain Accuracy = \",Accuracy)\n",
    "print(\"ClassifierChain Micro_Precision = \",Micro_Precision)\n",
    "print(\"ClassifierChain Micro_Recall = \",Micro_Recall)\n",
    "print(\"ClassifierChain Micro_F1 = \",Micro_F1)\n",
    "print(\"ClassifierChain Micro_Average_Prec = \",Micro_Average_Prec)\n",
    "print(\"ClassifierChain Macro_Precision = \",Macro_Precision)\n",
    "print(\"ClassifierChain Macro_Recall = \",Macro_Recall)\n",
    "print(\"ClassifierChain Macro_F1 = \",Macro_F1)\n",
    "print(\"ClassifierChain Macro_Average_Prec = \",Macro_Average_Prec)\n",
    "print(\"ClassifierChain Samples_Average_Prec = \",Samples_Average_Prec)\n",
    "print(\"ClassifierChain Weighted_Average_Prec = \",Weighted_Average_Prec)\n",
    "print(\"ClassifierChain Hamming_loss = \",Hamming_loss)\n",
    "print(\"ClassifierChain Ranking_loss = \",Ranking_loss)\n",
    "print(\"ClassifierChain Micro_Roc_auc = \",Micro_Roc_auc)\n",
    "print(\"ClassifierChain Coverage_error = \",Coverage_error)\n",
    "print(\"ClassifierChain label_ranking_average_precision_score = \",label_ranking_avg_prec_score)\n",
    "print(\"ClassifierChain Jaccard_score = \",Jaccard_score)\n",
    "print(\"ClassifierChain Jaccard_score_macro = \",Jaccard_score_macro)\n",
    "Eval6 = ['ClassifierChain',Accuracy,Micro_Precision,Micro_Recall,Micro_F1,Micro_Average_Prec,\n",
    "         Macro_Precision,Macro_Recall,Macro_F1,Macro_Average_Prec,Samples_Average_Prec,Weighted_Average_Prec,Hamming_loss,\n",
    "         Ranking_loss,Micro_Roc_auc,Coverage_error,label_ranking_avg_prec_score,Jaccard_score,Jaccard_score_macro]\n",
    "Evals.append(Eval6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5. Multiple Binary Classifications - (One Vs Rest Classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),])\n",
    "# Training logistic regression model on train data\n",
    "LogReg_pipeline.fit(X_train,y_train )\n",
    "# calculating test accuracy\n",
    "Logprediction = LogReg_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy   None, 'micro', 'macro', 'weighted', 'samples'\n",
    "Accuracy = accuracy_score(y_test,Logprediction)\n",
    "Micro_Precision = precision_score(y_test,Logprediction, average='micro')\n",
    "Micro_Recall = recall_score(y_test,Logprediction, average='micro')\n",
    "Micro_F1 = f1_score(y_test,Logprediction, average='micro')\n",
    "Micro_Average_Prec = average_precision_score(y_test, Logprediction, average='micro')\n",
    "Macro_Precision = precision_score(y_test,Logprediction, average='macro')\n",
    "Macro_Recall = recall_score(y_test,Logprediction, average='macro')\n",
    "Macro_F1 = f1_score(y_test,Logprediction, average='macro')\n",
    "Macro_Average_Prec = average_precision_score(y_test, Logprediction, average='macro')\n",
    "Samples_Average_Prec = average_precision_score(y_test, Logprediction, average='samples')\n",
    "Weighted_Average_Prec = average_precision_score(y_test, Logprediction, average='weighted')\n",
    "Hamming_loss = hamming_loss(y_test,Logprediction)\n",
    "Ranking_loss = label_ranking_loss(y_test, Logprediction)\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), Logprediction.ravel())\n",
    "Micro_Roc_auc = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "Coverage_error = coverage_error(y_test, Logprediction)\n",
    "label_ranking_avg_prec_score = label_ranking_average_precision_score(y_test, Logprediction)\n",
    "Jaccard_score = jaccard_score(y_test, Logprediction, average='samples')\n",
    "Jaccard_score_macro = jaccard_score(y_test, Logprediction, average='macro')\n",
    "print(\"OneVsRestClassifier_LogisticRegression Accuracy = \",Accuracy)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Micro_Precision = \",Micro_Precision)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Micro_Recall = \",Micro_Recall)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Micro_F1 = \",Micro_F1)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Micro_Average_Prec = \",Micro_Average_Prec)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Macro_Precision = \",Macro_Precision)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Macro_Recall = \",Macro_Recall)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Macro_F1 = \",Macro_F1)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Macro_Average_Prec = \",Macro_Average_Prec)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Samples_Average_Prec = \",Samples_Average_Prec)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Weighted_Average_Prec = \",Weighted_Average_Prec)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Hamming_loss = \",Hamming_loss)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Ranking_loss = \",Ranking_loss)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Micro_Roc_auc = \",Micro_Roc_auc)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Coverage_error = \",Coverage_error)\n",
    "print(\"OneVsRestClassifier_LogisticRegression label_ranking_average_precision_score = \",label_ranking_avg_prec_score)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Jaccard_score = \",Jaccard_score)\n",
    "print(\"OneVsRestClassifier_LogisticRegression Jaccard_score_macro = \",Jaccard_score_macro)\n",
    "Eval7 = ['OneVsRestClassifier_LogisticRegression',Accuracy,Micro_Precision,Micro_Recall,Micro_F1,Micro_Average_Prec,\n",
    "         Macro_Precision,Macro_Recall,Macro_F1,Macro_Average_Prec,Samples_Average_Prec,Weighted_Average_Prec,Hamming_loss,\n",
    "         Ranking_loss,Micro_Roc_auc,Coverage_error,label_ranking_avg_prec_score,Jaccard_score,Jaccard_score_macro]\n",
    "Evals.append(Eval7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Evals, columns=['Classifier','Accuracy','Micro_Pre','Micro_Recall','Micro_F1','Micro_Avg_Prec',\n",
    "         'Macro_Prec','Macro_Recall','Macro_F1','Macro_Avg_Prec','Samples_Avg_Prec','Weighted_Avg_Prec','Hamming_loss',\n",
    "         'Ranking_loss','Micro_Roc_auc','Coverage_error','label_ranking_avg_prec_score','Jaccard_score','Jaccard_score_macro'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the confusion matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "#print(multilabel_confusion_matrix(y_test, Logprediction))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_test, Logprediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.programcreek.com/python/example/81207/sklearn.metrics.roc_curve\n",
    "def print_roc(y_true, y_scores):\n",
    "        '''\n",
    "        Prints the ROC for this model.\n",
    "        '''\n",
    "        fpr, tpr, thresholds = roc_curve(y_test.toarray().ravel(), y_scores.ravel())\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)'% auc_score)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show() \n",
    "print_roc(y_test, Logprediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Binarize the output\n",
    "y = label_binarize(y, classes=[0, 1, 2])\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\n",
    "\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "                                 random_state=0))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "colors = cycle(['blue', 'red', 'green'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for multi-class data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
